{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34xRxfW8yWbw"
      },
      "outputs": [],
      "source": [
        "1.What does R-squared represent in a regression model?\n",
        "Ans:R-Squared (RÂ² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance\n",
        "in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression\n",
        "model (the goodness of fit)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. What are the assumptions of linear regression?\n",
        "Ans:Linear regression models are based on several assumptions, including:\n",
        "Linearity: The relationship between the independent (\\(X\\)) and dependent (\\(Y\\)) variables is linear\n",
        "Independence: Each observation is independent of the others\n",
        "Normality: The errors are normally distributed\n",
        "Homoscedasticity: The variance of the errors is constant across all values of the independent variable\n",
        "No missing values: All variables used in the model are relevant and have no missing values\n",
        "Sample size: The sample size is large enough, with at least 20 cases per independent variable"
      ],
      "metadata": {
        "id": "K91eFsUEzf9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.What is the difference between R-squared and Adjusted R-squared?\n",
        "Ans:While r-squared measures the proportion of variance in the dependent variable explained by the independent variables, it always increases\n",
        "when more predictors are added. Adjusted r-squared adjusts for the number of predictors and decreases if the additional variables do not contribute\n",
        "to the model's significance."
      ],
      "metadata": {
        "id": "muhQublw1j6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.Why do we use Mean Squared Error (MSE)?\n",
        "Ans:Mean Squared Error (MSE) is used to evaluate the quality of a model's predictions in regression tasks because it provides a clear measure of\n",
        "the average squared difference between predicted and actual values, effectively penalizing large errors more significantly, making it particularly\n",
        "useful when large deviations from the true value are critical to identify and minimize in a model."
      ],
      "metadata": {
        "id": "Mp4r9KYo29Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5.What does an Adjusted R-squared value of 0.85 indicate?\n",
        "Ans:For example, an R-Squared of 0.85 means 85% of the variance in your data is explained by the model. Low R-Squared: Your model doesn't fit\n",
        "well. However, don't rely solely on R Squared. Context and other diagnostic measures are crucial."
      ],
      "metadata": {
        "id": "di6ej02j3ciT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.How do we check for normality of residuals in linear regression?\n",
        "Ans:To check for normality of residuals in linear regression, you primarily use visual methods like a histogram of the residuals to see if it\n",
        "resembles a bell curve, and a quantile-quantile (Q-Q) plot to compare the quantiles of your residuals to a normal distribution; if the points\n",
        "fall roughly on a straight line, normality is supported; additionally, statistical tests like the Shapiro-Wilk test can be used to formally assess\n",
        "normality based on a p-value."
      ],
      "metadata": {
        "id": "Fznzt9oQ35mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.What is multicollinearity, and how does it impact regression?\n",
        "Ans:Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other, making it\n",
        "difficult to isolate the individual effect of each variable on the dependent variable, leading to unreliable and unstable estimates of regression\n",
        "coefficients; essentially, it means several variables are providing similar information, making it hard to determine which variable is truly\n",
        "impacting the outcome."
      ],
      "metadata": {
        "id": "WIxpn5cE35uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. What is Mean Absolute Error (MAE)?\n",
        "Ans:Mean Absolute Error (MAE) is a measure of the average size of the mistakes in a collection of predictions, without taking their direction\n",
        "into account. It is measured as the average absolute difference between the predicted values and the actual values and is used to assess the\n",
        "effectiveness of a regression model"
      ],
      "metadata": {
        "id": "xjmyr22n3510"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.What are the benefits of using an ML pipeline?\n",
        "Ans:Using an ML pipeline offers several key benefits, including: increased efficiency and productivity by automating tasks, improved\n",
        "reproducibility through standardized workflows, enhanced collaboration between team members, scalability to handle large datasets, flexibility\n",
        "to experiment with different model configurations, faster prediction generation, and better data quality through streamlined data preprocessing;\n",
        "all leading to quicker and more reliable machine learning model development and deployment."
      ],
      "metadata": {
        "id": "wONdgQkM357Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. Why is RMSE considered more interpretable than MSE?\n",
        "Ans:RMSE is in the same unit as the dependent variable, making it more interpretable than MSE."
      ],
      "metadata": {
        "id": "HTdA6fQk7LwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11.What is pickling in Python, and how is it useful in ML?\n",
        "Ans:In Python, \"pickling\" refers to the process of converting a Python object into a byte stream, essentially allowing you to save the object's\n",
        "state to a file so it can be loaded and used later; this is particularly useful in Machine Learning (ML) as it enables you to save trained models\n",
        "and reuse them without having to retrain them every time you need to make predictions."
      ],
      "metadata": {
        "id": "8eVezFeV7pS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. What does a high R-squared value mean?\n",
        "Ans:A high R-squared value indicates that a regression model explains a large proportion of the variability in the dependent variable, meaning\n",
        "the model fits the data well and does a good job of predicting the dependent variable based on the independent variables; essentially, a higher\n",
        "R-squared signifies a stronger relationship between the variables involved."
      ],
      "metadata": {
        "id": "5z1mGh_o8MaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. What happens if linear regression assumptions are violated?\n",
        "Ans:If the assumptions of linear regression are violated, the results of the model can be unreliable, biased, or misleading, leading to inaccurate\n",
        "predictions and incorrect inferences about the relationship between variables, as the model may not properly capture the true pattern in the data;\n",
        "this can include issues with confidence intervals, hypothesis tests, and the overall interpretation of the model coefficients.\n"
      ],
      "metadata": {
        "id": "pmtAZqMJ8Mmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14. How can we address multicollinearity in regression?\n",
        "Ans:To address multicollinearity in regression, you can: remove highly correlated variables from the model, combine correlated variables into a\n",
        "single composite variable, or use regularization techniques like ridge regression which can help stabilize the model coefficients even when variables\n",
        "are highly correlated; identifying the presence of multicollinearity often involves examining correlation matrices and Variance Inflation Factors\n",
        "(VIF) to pinpoint the problematic variables."
      ],
      "metadata": {
        "id": "E4XJ6sCc8MqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15.How can feature selection improve model performance in regression analysis?\n",
        "Ans:Feature selection can improve model performance in regression analysis by identifying and removing irrelevant or redundant features, which\n",
        "helps to reduce overfitting, increase the model's interpretability, and ultimately lead to better prediction accuracy on new data, especially\n",
        "when dealing with high-dimensional datasets."
      ],
      "metadata": {
        "id": "pimeWqtE8M6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16. How is Adjusted R-squared calculated?\n",
        "Ans:Adjusted R squared is calculated by dividing the residual mean square error by the total mean square error (which is the sample variance of\n",
        "the target field). The result is then subtracted from 1."
      ],
      "metadata": {
        "id": "aQdpSx1D8NCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. Why is MSE sensitive to outliers?\n",
        "Ans:Mean Squared Error (MSE) is sensitive to outliers because it squares the errors between predicted and actual values, meaning that large errors\n",
        "(like those caused by outliers) have a disproportionately large impact on the overall MSE, making it appear as if the model performs much worse\n",
        "than it actually does on the majority of data points."
      ],
      "metadata": {
        "id": "Uy5B0COVZlnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18.What is the role of homoscedasticity in linear regression?\n",
        "Ans:In a linear regression model, homoskedasticity occurs when the variance of the error term is constant. This indicates that the model is\n",
        "well-defined, meaning that the dependent variable is adequately defined by the predictor variable. If there is too much variance in the error term,\n",
        "the model isn't well-defined."
      ],
      "metadata": {
        "id": "Wd3PIiyjbJhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19.What is Root Mean Squared Error (RMSE)?\n",
        "Ans:The Root Mean Squared Error (RMSE) is one of the two main performance indicators for a regression model. It measures the average difference\n",
        "between values predicted by a model and the actual values. It provides an estimation of how well the model is able to predict the target value\n",
        "(accuracy)."
      ],
      "metadata": {
        "id": "QolkQ-PGZmqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20.Why is pickling considered risky?\n",
        "Ans:Pickling can be considered risky primarily because of the high salt content involved, which can contribute to health issues like high blood\n",
        "pressure, stomach upset, and potentially increased risk of stomach cancer, especially when consuming large quantities of heavily pickled foods;\n",
        "improper pickling techniques can also lead to the growth of harmful bacteria if not done with the correct acidity level and proper storage methods."
      ],
      "metadata": {
        "id": "ZrkTiiwOZsl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21.What alternatives exist to pickling for saving ML models?\n",
        "Ans:Serialization Formats\n",
        "Other serialization formats such as JSON, YAML, XML, and Protocol Buffers offer alternatives to pickle and joblib for saving models."
      ],
      "metadata": {
        "id": "sGntN32EZtbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22.What is heteroscedasticity, and why is it a problem?\n",
        "Ans:Heteroscedasticity refers to a situation in statistics where the variance of the errors (residuals) in a regression model is not constant\n",
        "across different values of the independent variable, meaning the spread of the data around the regression line varies depending on where you are\n",
        "on the x-axis; it's considered a problem because it violates a key assumption of linear regression, potentially leading to unreliable standard\n",
        "errors and hypothesis tests, even if the coefficient estimates themselves are unbiased."
      ],
      "metadata": {
        "id": "ACbg_Q5aZuNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23. How can interaction terms enhance a regression model's predictive power?\n",
        "Ans:Interaction terms in a regression model enhance predictive power by allowing the model to capture how the effect of one independent variable\n",
        "changes depending on the value of another independent variable, essentially creating a more nuanced and flexible relationship between the variables,\n",
        "leading to a better fit to the data and improved prediction accuracy."
      ],
      "metadata": {
        "id": "_VlFYW9FZu7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}